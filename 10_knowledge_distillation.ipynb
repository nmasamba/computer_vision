{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0c6839",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between it’s outputs and the teacher’s outputs, thus making it mimic the behavior. It was first introduced in a paper by Hinton, Vinyals and Dean: https://arxiv.org/abs/1503.02531. \n",
    "\n",
    "In this guide, we will do task-specific knowledge distillation. Specifically, this guide demonstrates how you can distill a fine-tuned ViT model (teacher model) to a MobileNet (student model) using the Trainer API of HF Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726400eb",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33823abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (4.45.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (2.15.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: accelerate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.25.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tensorboard in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (2.10.0)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: evaluate in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (0.4.1)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-macosx_10_15_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: xxhash in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: psutil in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Using cached grpcio-1.66.2.tar.gz (12.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nm/opt/anaconda3/envs/nlp/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Using cached accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached pyarrow-17.0.0-cp310-cp310-macosx_10_15_x86_64.whl (29.0 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: grpcio\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets accelerate tensorboard evaluate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import AutoImageProcessor, TrainingArguments, Trainer, \\\n",
    "AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc696824",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beans dataset. We'll use merve/beans-vit-224 model as teacher model. \n",
    "# It’s an image classification model, based on google/vit-base-patch16-224-in21k fine-tuned on beans dataset\n",
    "dataset = load_dataset(\"beans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61b908",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80834e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a preprocessor from either model\n",
    "\n",
    "teacher_processor = AutoImageProcessor.from_pretrained(\"merve/beans-vit-224\")\n",
    "\n",
    "def process(examples):\n",
    "    processed_inputs = teacher_processor(examples[\"image\"])\n",
    "    return processed_inputs\n",
    "\n",
    "# use the map() method of dataset to apply the preprocessing to every split of the dataset\n",
    "processed_datasets = dataset.map(process, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will distill merve/beans-vit-224 model to a randomly initialized MobileNetV2\n",
    "# We want the randomly initialized MobileNet to mimic the teacher model\n",
    "# To achieve this, (1) we first get the logits output from the teacher and the student, \n",
    "# (2) we divide each of them by the parameter temperature which controls the importance of each soft target,\n",
    "# (3) use the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher.\n",
    "# If the logits from the two models are identical, their KL divergence will equal zero.\n",
    "\n",
    "class ImageDistilTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.teacher.to(device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False):\n",
    "        student_output = self.student(**inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs)\n",
    "\n",
    "        # Compute soft targets for teacher and student\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "\n",
    "        # Compute the true label loss\n",
    "        student_target_loss = student_output.loss\n",
    "\n",
    "        # Calculate final loss\n",
    "        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n",
    "        return (loss, student_output) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36459c67",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"knowledge_distillation\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False\n",
    "    )\n",
    "\n",
    "num_labels = len(processed_datasets[\"train\"].features[\"labels\"].names)\n",
    "\n",
    "# initialize models\n",
    "teacher_model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"merve/beans-vit-224\",\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# training MobileNetV2 from scratch\n",
    "student_config = MobileNetV2Config()\n",
    "student_config.num_labels = num_labels\n",
    "student_model = MobileNetV2ForImageClassification(student_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39d42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddfa96a7",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f094c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec84c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bd147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
