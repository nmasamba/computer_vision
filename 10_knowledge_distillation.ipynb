{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a07f64",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between it’s outputs and the teacher’s outputs, thus making it mimic the behavior. It was first introduced in a paper by Hinton, Vinyals and Dean: https://arxiv.org/abs/1503.02531. \n",
    "\n",
    "In this guide, we will do task-specific knowledge distillation. Specifically, this guide demonstrates how you can distill a fine-tuned ViT model (teacher model) to a MobileNet (student model) using the Trainer API of HF Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa37fb",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets accelerate tensorboard evaluate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24325ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e956775",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beans dataset. We'll use merve/beans-vit-224 model as teacher model. \n",
    "# It’s an image classification model, based on google/vit-base-patch16-224-in21k fine-tuned on beans dataset\n",
    "dataset = load_dataset(\"beans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85133b9",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0527da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a preprocessor from either model\n",
    "\n",
    "teacher_processor = AutoImageProcessor.from_pretrained(\"merve/beans-vit-224\")\n",
    "\n",
    "def process(examples):\n",
    "    processed_inputs = teacher_processor(examples[\"image\"])\n",
    "    return processed_inputs\n",
    "\n",
    "# use the map() method of dataset to apply the preprocessing to every split of the dataset\n",
    "processed_datasets = dataset.map(process, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will distill merve/beans-vit-224 model to a randomly initialized MobileNetV2\n",
    "# We want the randomly initialized MobileNet to mimic the teacher model\n",
    "# To achieve this, (1) we first get the logits output from the teacher and the student, \n",
    "# (2) we divide each of them by the parameter temperature which controls the importance of each soft target,\n",
    "# (3) use the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher.\n",
    "# If the logits from the two models are identical, their KL divergence will equal zero.\n",
    "\n",
    "class ImageDistilTrainer(Trainer):\n",
    "    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.teacher.to(device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "    def compute_loss(self, student, inputs, return_outputs=False):\n",
    "        student_output = self.student(**inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs)\n",
    "\n",
    "        # Compute soft targets for teacher and student\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "\n",
    "        # Compute the true label loss\n",
    "        student_target_loss = student_output.loss\n",
    "\n",
    "        # Calculate final loss\n",
    "        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n",
    "        return (loss, student_output) if return_outputs else loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
