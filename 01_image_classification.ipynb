{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58e26ee",
   "metadata": {},
   "source": [
    "Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that comprise an image. There are many applications for image classification, such as detecting damage after a natural disaster, monitoring crop health, or helping screen medical images for signs of disease.\n",
    "\n",
    "This guide illustrates how to:\n",
    "\n",
    "1. Fine-tune ViT on the Food-101 dataset to classify a food item in an image.\n",
    "2. Use your fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd88d82",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72752e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b30a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, DefaultDataCollator\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce57af",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset for experimentation\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "\n",
    "# Split the datasetâ€™s train split into a train and test set \n",
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect example\n",
    "# Each example in the dataset has two fields:\n",
    "# image: a PIL image of the food item\n",
    "# label: the label class of the food item\n",
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary that maps the label name to an integer and vice versa\n",
    "# makes it easier for the model to get the label name from the label id,\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecab0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks: convert the label id to a label name and vice-versa\n",
    "id2label[str(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id['breakfast_burrito']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a121540",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image into a tensor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0339b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to make the model more robust against overfitting\n",
    "# torchvision.transforms module used here but any suitable library can be used\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "# Create a preprocessing function to apply transforms and return pixel_values to be used as model inputs\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Apply to dataset on the fly using with_transform\n",
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of examples using DefaultDataCollator\n",
    "# NB: DefaultDataCollator does not perform additional preprocessing such as dynamic padding\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c16034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
