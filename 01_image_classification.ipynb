{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1cda10",
   "metadata": {},
   "source": [
    "Image classification assigns a label or class to an image. Unlike text or audio classification, the inputs are the pixel values that comprise an image. There are many applications for image classification, such as detecting damage after a natural disaster, monitoring crop health, or helping screen medical images for signs of disease.\n",
    "\n",
    "This guide illustrates how to:\n",
    "\n",
    "1. Fine-tune ViT on the Food-101 dataset to classify a food item in an image.\n",
    "2. Use your fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d808d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b43bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import AutoImageProcessor, DefaultDataCollator, AutoModelForImageClassification, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33166e6e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9353d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset for experimentation\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "\n",
    "# Split the dataset’s train split into a train and test set \n",
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76bcb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect example\n",
    "# Each example in the dataset has two fields:\n",
    "# image: a PIL image of the food item\n",
    "# label: the label class of the food item\n",
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary that maps the label name to an integer and vice versa\n",
    "# makes it easier for the model to get the label name from the label id,\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks: convert the label id to a label name and vice-versa\n",
    "id2label[str(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36671233",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id['breakfast_burrito']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcff349",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image into a tensor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to make the model more robust against overfitting\n",
    "# torchvision.transforms module used here but any suitable library can be used\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "# Create a preprocessing function to apply transforms and return pixel_values to be used as model inputs\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Apply to dataset on the fly using with_transform\n",
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of examples using DefaultDataCollator\n",
    "# NB: DefaultDataCollator does not perform additional preprocessing such as padding\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4bd2f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2366da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that passes your predictions and labels to compute to calculate the accuracy\n",
    "# compute_metrics function ready to be called during training\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40016a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iniatestant model with number of labels along with the number of expected labels, and the label mappings\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f70002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training hyperparameters in TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"image_clf_model\",\n",
    "    # IMPORTANT: don’t remove unused columns because that’ll drop the image column\n",
    "    remove_unused_columns=False, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Pass the training arguments to Trainer along with the model \n",
    "# + dataset + tokenizer + data collator + compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
