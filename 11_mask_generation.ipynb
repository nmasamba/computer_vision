{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40738ff-a5de-451f-9080-b1fb7d292111",
   "metadata": {},
   "source": [
    "# Mask Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420d9df-94b3-4706-a611-4b49738279c9",
   "metadata": {},
   "source": [
    "Mask generation is the task of generating semantically meaningful masks for an image. This task is very similar to image segmentation, but there are differences between the two tasks. The most notable difference is while image segmentation models are trained on labeled datasets and are limited to the classes they have seen during training (they return a set of masks and corresponding classes, given an image), mask generation models are trained on large amounts of data and can be used to infer previously unseen masked segments of an image.\n",
    "\n",
    "Mask generation models operate in two modes.\n",
    "- Prompting mode: In this mode, the model takes in an image and a prompt, where a prompt can be a 2D point location (XY coordinates) in the image within an object or a bounding box surrounding an object. In prompting mode, the model only returns the mask over the object that the prompt is pointing out.\n",
    "- Segment Everything mode: In segment everything, given an image, the model generates every mask in the image. To do so, a grid of points is generated and overlaid on the image for inference.\n",
    "\n",
    "This mask generation task is supported by Segment Anything Model (based on https://arxiv.org/abs/2304.02643). Itâ€™s a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks. SAM serves as a powerful foundation model for segmentation as it has large data coverage. It is trained on SA-1B, a dataset with 1 million images and 1.1 billion masks.\n",
    "\n",
    "This guide teaches how to:\n",
    "1. Infer in Segment Everything mode with batching,\n",
    "2. Infer in Point Prompting mode,\n",
    "3. Infer in Box Prompting mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e30f1-b7d3-49a3-97aa-8b2184d1479c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09e079-16b9-4e44-8b21-6696597a0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768631-7ec2-400e-844d-5b66bd8022a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9247e4-3c42-4238-81d4-7dcef98ca26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import pipeline, SamModel, SamProcessor\n",
    "\n",
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09751dc0-5223-4b25-974e-aba1e00ed920",
   "metadata": {},
   "source": [
    "# Mask Generation using pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae10d2-fd68-4ebc-9a8b-b9d9ca6a4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/sam-vit-huge\"\n",
    "mask_generator = pipeline(model=checkpoint, task=\"mask-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7adfd-f7e4-4f0c-80f2-8bd0fbbd9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ee0cf-39e3-47cd-9f44-dc3985eb29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Everything (SE) mode\n",
    "# points-per-batch enables parallel inference in SE mode (faster inference, but consumes more memory) \n",
    "# pred_iou_thresh is the IoU confidence threshold where only the masks above that certain threshold are returnes\n",
    "# NB: SAM only enables batching over points and not the images. \n",
    "masks = mask_generator(image, points_per_batch=128, pred_iou_thresh=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb2da0-6e81-418b-b14c-a3d0bdb1b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the mask\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "for i, mask in enumerate(masks[\"masks\"]):\n",
    "    plt.imshow(mask, cmap='viridis', alpha=0.1, vmin=0, vmax=1)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a9bd1-13f1-438d-9a3f-bc7adb68b7d5",
   "metadata": {},
   "source": [
    "# Mask Generation manual process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f39daf-6961-4338-a505-1810da1c7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae5d86-ca6d-40c8-a813-8e8fb8eac12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point prompting mode\n",
    "# Pass the input point to the processor, then take the processor output and pass it to the model for inference\n",
    "input_points = [[[2592, 1728]]] # point location of the bee\n",
    "\n",
    "inputs = processor(image, input_points=input_points, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "# Visualize the three masks in the masks output\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original Image')\n",
    "mask_list = [masks[0][0][0].numpy(), masks[0][0][1].numpy(), masks[0][0][2].numpy()]\n",
    "\n",
    "for i, mask in enumerate(mask_list, start=1):\n",
    "    overlayed_image = np.array(image).copy()\n",
    "\n",
    "    overlayed_image[:,:,0] = np.where(mask == 1, 255, overlayed_image[:,:,0])\n",
    "    overlayed_image[:,:,1] = np.where(mask == 1, 0, overlayed_image[:,:,1])\n",
    "    overlayed_image[:,:,2] = np.where(mask == 1, 0, overlayed_image[:,:,2])\n",
    "    \n",
    "    axes[i].imshow(overlayed_image)\n",
    "    axes[i].set_title(f'Mask {i}')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deaf64-df54-4c58-8616-ff477853347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box prompting mode\n",
    "# Pass the input box in the format of a list [x_min, y_min, x_max, y_max] format along with the image to the processor\n",
    "# Below box is the bounding box around the bee\n",
    "box = [2350, 1600, 2850, 2100]\n",
    "\n",
    "inputs = processor(\n",
    "        image,\n",
    "        input_boxes=[[[box]]],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "# Take the processor output and directly pass it to the model, then post-process the output again\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "mask = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.cpu(),\n",
    "    inputs[\"original_sizes\"].cpu(),\n",
    "    inputs[\"reshaped_input_sizes\"].cpu()\n",
    ")[0][0][0].numpy()\n",
    "\n",
    "# Visualise the bounding box around the bee\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "\n",
    "rectangle = patches.Rectangle((2350, 1600), 500, 500, linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rectangle)\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d628010-5c39-4770-a5b9-672f9987cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box prompting inference output\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "ax.imshow(mask, cmap='viridis', alpha=0.4)\n",
    "\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
